{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn-pong-v3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrumst/ML/blob/master/dqn_pong_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_0aXOLnA5is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random\n",
        "\n",
        "UP_ACTION = 2\n",
        "DOWN_ACTION = 3\n",
        "\n",
        "env = gym.make(\"Pong-v0\")\n",
        "next_state = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veSqvFfdCpoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAX8BNx_BRpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.wrappers import Monitor\n",
        "env_sim = Monitor(env, './video', force=True)\n",
        "\n",
        "_ = env_sim.reset()\n",
        "for i in range(500):\n",
        "    action = random.randint(UP_ACTION, DOWN_ACTION)\n",
        "    next_state, reward, done, info = env_sim.step(action)\n",
        "    if done:\n",
        "        env_sim.reset()\n",
        "\n",
        "env_sim.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QuzHgzYDeat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "    import glob\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "            loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxl-kyO1D74n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary modules from keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "# creates a generic neural network architecture\n",
        "model = Sequential()\n",
        "\n",
        "# hidden layer takes a pre-processed frame as input, and has 200 units\n",
        "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
        "\n",
        "# compile the model using traditional Machine Learning losses and optimizers\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FdwI7X9IFBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
        "def prepro(img):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "    img = img[35:195] # crop\n",
        "    img = img[::2,::2,0] # downsample by factor of 2\n",
        "    img[img == 144] = 0 # erase background (background type 1)\n",
        "    img[img == 109] = 0 # erase background (background type 2)\n",
        "    img[img != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    return img.astype(np.float).ravel()\n",
        "\n",
        "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
        "def discount_rewards(r, gamma):\n",
        "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "    r = np.array(r)\n",
        "    discounted_r = np.zeros_like(r)\n",
        "    running_add = 0\n",
        "    # we go from last reward to first one so we don't have to do exponentiations\n",
        "    for t in reversed(range(0, r.size)):\n",
        "        if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
        "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
        "        discounted_r[t] = running_add\n",
        "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
        "    discounted_r /= np.std(discounted_r) #idem\n",
        "    return discounted_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY1_amzhHR07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_state = env.reset()\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "rewards = []\n",
        "total_reward = 0\n",
        "\n",
        "episode_nb = 0\n",
        "prev_input = None\n",
        "cur_input = None\n",
        "\n",
        "gamma = .9\n",
        "\n",
        "for i in range(50000):\n",
        "    cur_input = prepro(next_state)\n",
        "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
        "    prev_input = cur_input\n",
        "\n",
        "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
        "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
        "    y = 1 if action == UP_ACTION else 0 # 0 and 1 are our labels\n",
        "\n",
        "    # log the input and label to train later\n",
        "    x_train.append(x)\n",
        "    y_train.append(y)\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "        print('At the end of episode', episode_nb, 'the total reward was :', total_reward)\n",
        "        # increment episode number\n",
        "        episode_nb += 1\n",
        "        # training\n",
        "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
        "\n",
        "        x_train, y_train, rewards = [], [], []\n",
        "        total_reward = 0\n",
        "        prev_input = None\n",
        "        next_state = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}