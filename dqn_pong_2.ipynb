{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn-pong-#2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrumst/ML/blob/master/dqn_pong_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipwdFHUVB5qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WoSzJUgB5qJ",
        "colab_type": "text"
      },
      "source": [
        "## Load GYM environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RO8UWiE4B5qK",
        "colab_type": "code",
        "outputId": "69ac0180-4dbe-455e-fb05-8278f5a27d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "env = gym.make('Pong-v0')\n",
        "\n",
        "STATE_SHAPE = env.observation_space.shape\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "\n",
        "print('Actions: {}'.format(NUM_ACTIONS))\n",
        "print('States shape: {}'.format(STATE_SHAPE))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actions: 6\n",
            "States shape: (210, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tcQQwNh7nUZ",
        "colab_type": "code",
        "outputId": "f7cd2e59-162b-436b-a96e-c19e9a863d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "env.__dict__"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_elapsed_steps': None,\n",
              " '_max_episode_steps': 10000,\n",
              " 'action_space': Discrete(6),\n",
              " 'env': <gym.envs.atari.atari_env.AtariEnv at 0x7f0e1b7610b8>,\n",
              " 'metadata': {'render.modes': ['human', 'rgb_array']},\n",
              " 'observation_space': Box(210, 160, 3),\n",
              " 'reward_range': (-inf, inf)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNiSdKeo8I4O",
        "colab_type": "code",
        "outputId": "ff98eea4-5382-47af-bb97-5e55f2902657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.observation_space.shape[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHaEq-ExGjp5",
        "colab_type": "text"
      },
      "source": [
        "## Example of a state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUcBEkqTwWL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00e1f870-bdc1-4ec2-aae7-c60fc6b410f4"
      },
      "source": [
        "def epsilon_factory(arg_delta, min_num):\n",
        "    def get_epsilon_for_iteration(i):\n",
        "        y = (arg_delta ** -i) + min_num\n",
        "        return y if y <= 1.0 else 1.0\n",
        "    return get_epsilon_for_iteration\n",
        "\n",
        "epsilon_factory(1.005, .25)(100)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8572867761711169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HycCupurCLXM",
        "colab_type": "code",
        "outputId": "c02a5971-d326-413a-b0f4-cbf214c50923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "s = env.reset()\n",
        "_=plt.imshow(s)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUSl-luBMors",
        "colab_type": "text"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAg6k47P9G1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "cbbd0b4d-36a3-4245-f6d8-31e84cc1fc39"
      },
      "source": [
        "def atari_model(n_actions):\n",
        "    # We assume a theano backend here, so the \"channels\" are first.\n",
        "    ATARI_SHAPE = (4, 105, 80)\n",
        "\n",
        "    # With the functional API we need to define the inputs.\n",
        "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
        "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
        "\n",
        "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
        "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
        "    \n",
        "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
        "    conv_1 = keras.layers.convolutional.Convolution2D(\n",
        "        16, 4, 4, subsample=(, 4), activation='relu', name='conv_1'\n",
        "    )(normalized)\n",
        "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
        "    conv_2 = keras.layers.convolutional.Convolution2D(\n",
        "        32, 4, 4, subsample=(2, 2), activation='relu', name='conv_2'\n",
        "    )(conv_1)\n",
        "    # Flattening the second convolutional layer.\n",
        "    conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
        "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
        "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
        "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
        "    output = keras.layers.Dense(n_actions)(hidden)\n",
        "    # Finally, we multiply the output by the mask!\n",
        "    filtered_output = keras.layers.merge([output, actions_input], mode='mul')\n",
        "\n",
        "    self.model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
        "    optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
        "    self.model.compile(optimizer, loss='mse')\n",
        "\n",
        "model = atari_model(2)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"conv_1\", strides=(2, 2))`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), activation=\"relu\", name=\"conv_2\", strides=(2, 2))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1606\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1607\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 4 from 2 for 'conv_2_5/convolution' (op: 'Conv2D') with input shapes: [?,2,53,16], [4,4,16,32].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-373b9389fd79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-373b9389fd79>\u001b[0m in \u001b[0;36matari_model\u001b[0;34m(n_actions)\u001b[0m\n\u001b[1;32m     17\u001b[0m     conv_2 = keras.layers.convolutional.Convolution2D(\n\u001b[1;32m     18\u001b[0m         \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv_2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     )(conv_1)\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Flattening the second convolutional layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mconv_flattened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             outputs = K.conv3d(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m         data_format=tf_data_format)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format, filters, dilations)\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution)\u001b[0m\n\u001b[1;32m   1007\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1010\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[1;32m   1072\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    792\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    793\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3355\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input #%d is not a tensor: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3356\u001b[0m     return self._create_op_internal(op_type, inputs, dtypes, input_types, name,\n\u001b[0;32m-> 3357\u001b[0;31m                                     attrs, op_def, compute_device)\n\u001b[0m\u001b[1;32m   3358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3359\u001b[0m   def _create_op_internal(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3424\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3426\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3427\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1768\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1769\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1770\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1771\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 2 for 'conv_2_5/convolution' (op: 'Conv2D') with input shapes: [?,2,53,16], [4,4,16,32]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMKyAI5GLN1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Lambda, Dense, merge\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.core import Flatten\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "# Train a DQN model here\n",
        "\n",
        "def to_grayscale(img):\n",
        "    return np.mean(img, axis=2).astype(np.uint8)\n",
        "\n",
        "def downsample(img):\n",
        "    return img[::2, ::2]\n",
        "\n",
        "def preprocess(img):\n",
        "    return to_grayscale(downsample(img))\n",
        "\n",
        "# Tools\n",
        "def transform_reward(reward):\n",
        "    return np.sign(reward)\n",
        "\n",
        "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
        "    \"\"\"Do one deep Q learning iteration.\n",
        "    \n",
        "    Params:\n",
        "    - model: The DQN\n",
        "    - gamma: Discount factor (should be 0.99)\n",
        "    - start_states: numpy array of starting states\n",
        "    - actions: numpy array of one-hot encoded actions corresponding to the start states\n",
        "    - rewards: numpy array of rewards corresponding to the start states and actions\n",
        "    - next_states: numpy array of the resulting states corresponding to the start states and actions\n",
        "    - is_terminal: numpy boolean array of whether the resulting state is terminal\n",
        "    \n",
        "    \"\"\"\n",
        "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
        "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
        "    # The Q values of the terminal states is 0 by definition, so override them\n",
        "    next_Q_values[is_terminal] = 0\n",
        "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
        "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
        "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
        "    # the targets by the actions.\n",
        "    model.fit(\n",
        "        [start_states, actions], actions * Q_values[:, None],\n",
        "        nb_epoch=1, batch_size=len(start_states), verbose=0\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiqA5AZNt1ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_epsilon_for_iteration = epsilon_factory(1.005, .25)\n",
        "\n",
        "def choose_best_action(model, state):\n",
        "    return model.predict([state, np.ones(actions.shape)])\n",
        "\n",
        "def q_iteration(env, model, state, iteration, memory):\n",
        "    # Choose epsilon based on the iteration\n",
        "    epsilon = get_epsilon_for_iteration(iteration)\n",
        "\n",
        "    # Choose the action \n",
        "    if random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = choose_best_action(model, state)\n",
        "\n",
        "    # Play one game iteration (note: according to the next paper, you should actually play 4 times here)\n",
        "    new_frame, reward, is_done, _ = env.step(action)\n",
        "    memory.add(state, action, new_frame, reward, is_done)\n",
        "\n",
        "    # Sample and fit\n",
        "    batch = memory.sample_batch(32)\n",
        "    fit_batch(model, batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IKRRutttJw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RingBuf:\n",
        "    def __init__(self, size):\n",
        "        # Pro-tip: when implementing a ring buffer, always allocate one extra element,\n",
        "        # this way, self.start == self.end always means the buffer is EMPTY, whereas\n",
        "        # if you allocate exactly the right number of elements, it could also mean\n",
        "        # the buffer is full. This greatly simplifies the rest of the code.\n",
        "        self.data = []\n",
        "        self.start = 0\n",
        "        self.end = 0\n",
        "        \n",
        "    def append(self, element):\n",
        "        self.data.append(element)\n",
        "\n",
        "    def sample_batch(self, size):\n",
        "        if len(self.data) < size:\n",
        "            return self.data\n",
        "        limit = random.randint(0, len(self.data) - size)\n",
        "        return self.data[limit:limit+size]\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[(self.start + idx) % len(self.data)]\n",
        "    \n",
        "    def __len__(self):\n",
        "        if self.end < self.start:\n",
        "            return self.end + len(self.data) - self.start\n",
        "        else:\n",
        "            return self.end - self.start\n",
        "        \n",
        "    def __iter__(self):\n",
        "        for i in range(len(self)):\n",
        "            yield self[i]\n",
        "\n",
        "# rbf = RingBuf(40)\n",
        "# for i in range(40):\n",
        "#     rbf.append((1,2,3))\n",
        "#     print(rbf.sample_batch(32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiOBPXjVGnLR",
        "colab_type": "text"
      },
      "source": [
        "## Virtual display and video recording for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzmt2e5jEbX6",
        "colab_type": "code",
        "outputId": "85deda89-733b-4996-bd94-b13858c99c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "RECORD_VIDEO = True\n",
        "\n",
        "if RECORD_VIDEO:\n",
        "  \n",
        "    !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "    from pyvirtualdisplay import Display\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sx3L1j9B5qn",
        "colab_type": "text"
      },
      "source": [
        "## Run simulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhSeQkSBB5qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if RECORD_VIDEO:\n",
        "    from gym.wrappers import Monitor\n",
        "    env_sim = Monitor(env, './video', force=True)\n",
        "else:\n",
        "    env_sim = env\n",
        "\n",
        "state = env_sim.reset()\n",
        "totalReward = 0\n",
        "\n",
        "UP_ACTION = 2\n",
        "DOWN_ACTION = 3\n",
        "\n",
        "for _ in range(1000):\n",
        "    env_sim.render()\n",
        "    \n",
        "    q = model.predict(state)\n",
        "    state, reward, done, _ = env_sim.step(np.argmax(q[0]))\n",
        "    totalReward += reward\n",
        "    if done:        \n",
        "        break\n",
        "    \n",
        "    if not RECORD_VIDEO:\n",
        "        time.sleep(1./30)\n",
        "        \n",
        "env_sim.close()\n",
        "\n",
        "print('Total reward = {}'.format(totalReward))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWteeIo8K2jR",
        "colab_type": "text"
      },
      "source": [
        "## Show the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEb1-4mhEU0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "    import glob\n",
        "    import io\n",
        "    import base64\n",
        "    from IPython.display import HTML\n",
        "    from IPython import display as ipythondisplay\n",
        "\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "            loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8eb7loItDM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}